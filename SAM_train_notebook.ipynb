{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuHDl7yyITqH"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TPNLXRxIbtG",
        "outputId": "917f27f5-f5ca-49be-b33f-e8a30e9a7428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  Experiments  periodic_behavior_bn_wd  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1sw6erqjaUU",
        "outputId": "e73bb6ce-d90a-4c18-fdbd-2ee6a04b203f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/tipt0p/periodic_behavior_bn_wd.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUVGggTfjes8",
        "outputId": "9ef3041e-72c2-4897-fb99-aadd809bdc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'periodic_behavior_bn_wd' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python periodic_behavior_bn_wd/run_train_and_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUm5cBJDImgw",
        "outputId": "b44bf843-a9a6-4ee8-fb30-654ebedca0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: activate: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls periodic_behavior_bn_wd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83KV0SqnIsbZ",
        "outputId": "516fca25-6f46-4f1a-b135-0c16446117c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/   fake.py            \u001b[01;34mnets\u001b[0m/               README.md\n",
            "\u001b[01;34mCIFAR10\u001b[0m/       get_info_funcs.py  parser_get_info.py  \u001b[01;32mrun_train_and_test.py\u001b[0m*\n",
            "cycle_env.yml  get_info.py        parser_train.py     training_utils.py\n",
            "data.py        LICENSE            Plots.ipynb         train.py\n",
            "\u001b[01;34mExperiments\u001b[0m/   nb_utils.py        \u001b[01;34m__pycache__\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1FzgOIPI6i2",
        "outputId": "f34c5b39-1c35-403d-daa1-513c403475ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  Experiments  periodic_behavior_bn_wd  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls periodic_behavior_bn_wd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oMNEGpcI_Qs",
        "outputId": "357080e1-19d8-4ada-9d43-e474d42e825e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints    fake.py\t\t  nets\t\t      README.md\n",
            "CIFAR10        get_info_funcs.py  parser_get_info.py  run_train_and_test.py\n",
            "cycle_env.yml  get_info.py\t  parser_train.py     training_utils.py\n",
            "data.py        LICENSE\t\t  Plots.ipynb\t      train.py\n",
            "Experiments    nb_utils.py\t  __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"./periodic_behavior_bn_wd\")"
      ],
      "metadata": {
        "id": "9C17_6C0JA99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQi8hBelJFl-",
        "outputId": "29892479-f7db-45c0-9a72-a7f7cd6a2d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/   fake.py            \u001b[01;34mnets\u001b[0m/               README.md\n",
            "\u001b[01;34mCIFAR10\u001b[0m/       get_info_funcs.py  parser_get_info.py  \u001b[01;32mrun_train_and_test.py\u001b[0m*\n",
            "cycle_env.yml  get_info.py        parser_train.py     training_utils.py\n",
            "data.py        LICENSE            Plots.ipynb         train.py\n",
            "\u001b[01;34mExperiments\u001b[0m/   nb_utils.py        \u001b[01;34m__pycache__\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python ./run_train_and_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6-dxbD4JKG0",
        "outputId": "fa81a6d8-2336-4b94-f831-e45817ff5a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: activate: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Managing experiments')\n",
        "parser.add_argument('--test', action='store_true',\n",
        "                        help='print (test) or os.system (run)')\n",
        "parser.add_argument('-f', help='service')\n",
        "\n",
        "args = parser.parse_args()\n",
        "    \n",
        "if args.test:\n",
        "    action = print\n",
        "else:\n",
        "    action = os.system\n",
        "\n",
        "ENVIRONMENT = 'CycleBNNet_env'\n",
        " \n",
        "data = 'CIFAR10'#'CIFAR10''CIFAR100'\n",
        "network = 'ConvNetSI'#'ResNet18SI''ConvNetSI''ResNet18SIAf''ConvNetSIAf'\n",
        "fix_noninvlr = 0.0\n",
        "\n",
        "save_path = './Experiments/{}_{}/'.format(network,data)\n",
        "if fix_noninvlr >=0:\n",
        "    save_path = './Experiments/{}_{}_noninvlr_{}/'.format(network,data,fix_noninvlr) \n",
        "if not os.path.exists('./Experiments'):\n",
        "    os.mkdir('./Experiments')\n",
        "if not os.path.exists(save_path):\n",
        "    os.mkdir(save_path)\n",
        "\n",
        "params = {'dataset' : data,\n",
        "          'model': network,\n",
        "          'noninvlr':fix_noninvlr,\n",
        "          'momentum': 0.0,\n",
        "          'num_channels':32,\n",
        "          'depth':3,# work only for ConvNet\n",
        "          'epochs': 1001,\n",
        "          'corrupt_train': 0.0,\n",
        "          'save_freq': 1,\n",
        "          'eval_freq':1000,\n",
        "          'use_data_size':50000,\n",
        "          'dir': save_path + 'checkpoints',\n",
        "          'init_scale':10.,\n",
        "          'fix_si_pnorm_value':-1,\n",
        "          'gpu':0\n",
        "         }\n",
        "\n",
        "lrs = [0.01,]    \n",
        "wds = [0.001,]\n",
        "\n",
        "add_params = '--use_test --no_schedule --no_aug'#--fbgd --fix_si_pnorm\n",
        "\n",
        "params_test = {'dataset' : data,\n",
        "          'model': network,\n",
        "          'num_channels': params['num_channels'],\n",
        "          'depth': params['depth'],\n",
        "          'init_scale':params['init_scale'],\n",
        "          'save_path': save_path + 'info',\n",
        "          'models_dir': save_path + 'checkpoints',\n",
        "          'use_data_size':params['use_data_size'],\n",
        "          'gpu':params['gpu']\n",
        "         }\n",
        "\n",
        "log_path = save_path + 'logs/'\n",
        "if not os.path.exists(save_path):\n",
        "    os.mkdir(save_path)\n",
        "if not os.path.exists(log_path):\n",
        "    os.mkdir(log_path)\n",
        "    \n",
        "info_path = save_path + 'info/'\n",
        "if not os.path.exists(info_path):\n",
        "    os.mkdir(info_path)\n",
        "\n",
        "commands = []\n",
        "\n",
        "for ind in range(len(lrs)):\n",
        "    p = params.copy()\n",
        "    p['lr_init'] = lrs[ind]\n",
        "    p['wd'] = wds[ind]\n",
        "\n",
        "    p_test = params_test.copy()\n",
        "\n",
        "    exp_name = 'c{}_d{}_ds{}_lr{}_wd{}_mom{}_corr{}_epoch{}'.format(p['num_channels'],p['depth'],p['use_data_size'],p['lr_init'],p['wd'],p['momentum'],p['corrupt_train'],p['epochs'])\n",
        "    if 'no_schedule' in add_params:\n",
        "        exp_name = exp_name + '_nosch'\n",
        "    if p['init_scale'] >0:\n",
        "        exp_name = exp_name + 'initscale{}'.format(p['init_scale'])\n",
        "    if 'no_aug' in add_params:\n",
        "        exp_name = exp_name + '_noaug'\n",
        "    if 'fbgd' in add_params:\n",
        "        exp_name = exp_name + '_fbgd'\n",
        "    if 'fix_si_pnorm' in add_params:\n",
        "        exp_name = exp_name + '_fix_si_pnorm{}'.format(p['fix_si_pnorm_value'])\n",
        "        \n",
        "\n",
        "    p['dir'] = params['dir'] + '/' + exp_name\n",
        "    exp_log_path = log_path + exp_name\n",
        "\n",
        "    p_test['models_dir'] = params_test['models_dir'] + '/' + exp_name + '/trial_0'\n",
        "\n",
        "    # train\n",
        "    command = 'train.py {} >> {}'.format(' '.join([\"--{} {}\".format(k,v) for (k, v) in p.items()])+' ' +add_params, exp_log_path+'.out')\n",
        "    commands.append(command)\n",
        "\n",
        "    #train metrics\n",
        "    p_test['save_path'] = params_test['save_path'] + '/' + exp_name + '/train-tm.npz'\n",
        "    commands.append('get_info.py {} --corrupt_train {} --train_mode --eval_model --all_pnorm'.format(' '.join([\"--{} {}\".format(k,v) for (k, v) in p_test.items()]), p['corrupt_train']))\n",
        "    commands.append('get_info.py {} --corrupt_train {} --train_mode --update  --calc_grad_norms'.format(' '.join([\"--{} {}\".format(k,v) for (k, v) in p_test.items()]), p['corrupt_train']))\n",
        "\n",
        "    #test metrics\n",
        "    p_test['save_path'] = params_test['save_path'] + '/' + exp_name + '/test-em.npz'\n",
        "    commands.append('get_info.py {} --use_test --eval_model'.format(' '.join([\"--{} {}\".format(k,v) for (k, v) in p_test.items()])))\n",
        "           \n",
        "                    \n",
        "if ENVIRONMENT:\n",
        "    tmp_str = ' && ~/anaconda3/envs/{}/bin/python '.format(ENVIRONMENT)\n",
        "    final_command = \"bash -c '. activate {} {} {}'\".format(ENVIRONMENT,tmp_str,tmp_str.join(commands))\n",
        "else:\n",
        "    final_command = 'python '.join(command)\n",
        "\n",
        "\n",
        "                  "
      ],
      "metadata": {
        "id": "2g-J3coVkIvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "def parser():\n",
        "    parser = argparse.ArgumentParser(description=\"Model training\")\n",
        "    \n",
        "    parser.add_argument(\n",
        "        '--gpu', \n",
        "        type=str,\n",
        "        default='0',\n",
        "        help=\"GPU to use\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--dir\",\n",
        "        type=str,\n",
        "        default=\"./checkpoints\",\n",
        "        required=False,\n",
        "        help=\"training directory (default: None)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--dataset\", \n",
        "        type=str, \n",
        "        default=\"CIFAR10\", \n",
        "        help=\"dataset name (default: CIFAR10)\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--data_path\",\n",
        "        type=str,\n",
        "        default=\"./CIFAR10\",\n",
        "        metavar=\"PATH\",\n",
        "        help=\"path to datasets location (default: ~/datasets/)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--use_test\",\n",
        "        dest=\"use_test\",\n",
        "        action=\"store_true\",\n",
        "        help=\"use test dataset instead of validation (default: False)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--corrupt_train\", \n",
        "        type=float, \n",
        "        default=None,\n",
        "        help=\"train data corruption fraction (default: None --- no corruption)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--split_classes\", \n",
        "        type=int, \n",
        "        default=None,\n",
        "        help=\"split classes for CIFAR-10 (default: None --- no split)\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fbgd\",\n",
        "        dest=\"fbgd\",\n",
        "        action=\"store_true\",\n",
        "        help=\"train with full-batch GD (default: False)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--batch_size\",\n",
        "        type=int,\n",
        "        default=128,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size (default: 128)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--num_workers\",\n",
        "        type=int,\n",
        "        default=4,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of workers (default: 4)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--model\",\n",
        "        type=str,\n",
        "        default=\"ConvNetSI\",\n",
        "        required=False,\n",
        "        metavar=\"MODEL\",\n",
        "        help=\"model name (default: None)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--trial\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"trial number (default: 0)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--resume_epoch\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        metavar=\"CKPT\",\n",
        "        help=\"checkpoint epoch to resume training from (default: -1 --- start from scratch)\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=1001,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 1001)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--use_data_size\",\n",
        "        type=int,\n",
        "        default=None,\n",
        "        help=\"how many examples to use for training\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--save_freq\",\n",
        "        type=int,\n",
        "        default=None, \n",
        "        metavar=\"N\",\n",
        "        help=\"save frequency (default: None --- custom saving)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--save_freq_int\",\n",
        "        type=int,\n",
        "        default=0, \n",
        "        metavar=\"N\",\n",
        "        help=\"internal save frequency - how many times to save at each epoch (default: None --- custom saving)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--eval_freq\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"evaluation frequency (default: 10)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--lr_init\",\n",
        "        type=float,\n",
        "        default=0.1,\n",
        "        metavar=\"LR\",\n",
        "        help=\"initial learning rate (default: 0.01)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--noninvlr\",\n",
        "        type=float,\n",
        "        default=-1,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate for not scale-inv parameters\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--momentum\",\n",
        "        type=float,\n",
        "        default=0.9,\n",
        "        metavar=\"M\",\n",
        "        help=\"SGD momentum (default: 0.9)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--wd\", \n",
        "        type=float, \n",
        "        default=1e-4, \n",
        "        help=\"weight decay (default: 1e-4)\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--loss\",\n",
        "        type=str,\n",
        "        default=\"CE\",\n",
        "        help=\"loss to use for training model (default: Cross-entropy)\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--seed\", \n",
        "        type=int, \n",
        "        default=1, \n",
        "        metavar=\"S\", \n",
        "        help=\"random seed (default: 1)\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--num_channels\", \n",
        "        type=int, \n",
        "        default=64, \n",
        "        help=\"number of channels for resnet\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--depth\", \n",
        "        type=int, \n",
        "        default=3, \n",
        "        help=\"depth of convnet\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--scale\", \n",
        "        type=int, \n",
        "        default=25, \n",
        "        help=\"scale of lenet\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--no_schedule\", \n",
        "        default=True,\n",
        "        action=\"store_true\", \n",
        "        help=\"disable lr schedule\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--c_schedule\", \n",
        "        type=float,\n",
        "        default=None, \n",
        "        help=\"continuous schedule - decrease lr linearly after 1/4 epochs so that at the end it is x times lower \"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--d_schedule\", \n",
        "        type=float,\n",
        "        default=None, \n",
        "        help=\"discrete schedule - decrease lr x times after each 1/4 epochs \"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--init_scale\", \n",
        "        type=float,\n",
        "        default=-1, \n",
        "        help=\"init norm of the last layer weights \"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--no_aug\", \n",
        "        action=\"store_true\", \n",
        "        help=\"disable data augmentation\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--fix_si_pnorm\", \n",
        "        action=\"store_true\", \n",
        "        help=\"set SI-pnorm to init after each iteration\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--fix_si_pnorm_value\", \n",
        "        type=float, \n",
        "        default=-1,\n",
        "        help=\"custom fixed SI-pnorm value (must go with --fix_si_pnorm flag; default: -1 --- use init SI-pnorm value)\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--cosan_schedule\", \n",
        "        action=\"store_true\", \n",
        "        help=\"cosine anealing schedule\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-f\",  \n",
        "        help=\"google colab arg\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "id": "T3C20EuwmUMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import time\n",
        "import tabulate\n",
        "import data\n",
        "import training_utils\n",
        "import nets as models\n",
        "\n",
        "columns = [\"ep\", \"lr\", \"tr_loss\", \"tr_acc\", \"weights_norm\", \"te_loss\", \"te_acc\", \"time\"]\n",
        "\n",
        "def cross_entropy(model, input, target, reduction=\"mean\"):\n",
        "    \"standard cross-entropy loss function\"\n",
        "    output = model(input)\n",
        "    loss = F.cross_entropy(output, target, reduction=reduction)\n",
        "    return loss, output\n",
        "\n",
        "def check_si_name(n, model_name='ResNet18'):\n",
        "    if model_name == 'ResNet18':\n",
        "        return \"conv1\" in n or \"1.bn1\" in n or \"1.0.bn1\" in n or ((\"conv2\" in n or \"short\" in n) and \"4\" not in n)\n",
        "    elif model_name == 'ResNet18SI':\n",
        "        return 'linear' not in n\n",
        "    elif model_name == 'ResNet18SIAf':\n",
        "        return ('linear' not in n and 'bn' not in n and 'shortcut.0' not in n)\n",
        "    elif 'ConvNet' in model_name:\n",
        "        return 'conv_layers.0.' in n or 'conv_layers.3.' in n or 'conv_layers.7.' in n or 'conv_layers.11.' in n\n",
        "    return False\n",
        "    \n",
        "def main():\n",
        "    \n",
        "    args = parser()\n",
        "    args.device = None\n",
        "    \n",
        "    os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
        "    os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        args.device = torch.device(\"cuda\")\n",
        "        args.cuda = True\n",
        "    else:\n",
        "        args.device = torch.device(\"cpu\")\n",
        "        args.cuda = False\n",
        "  \n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    # n_trials = 1\n",
        "    \n",
        "    print(\"Preparing base directory %s\" % args.dir)\n",
        "    os.makedirs(args.dir, exist_ok=True)\n",
        "\n",
        "    # for trial in range(n_trials):\n",
        "    trial = args.trial\n",
        "    output_dir = args.dir + f\"/trial_{trial}\"\n",
        "    \n",
        "    ### resuming is modified!!!\n",
        "    if args.resume_epoch > -1:\n",
        "        resume_dir = output_dir\n",
        "        output_dir = output_dir + f\"/from_{args.resume_epoch}_for_{args.epochs}\"\n",
        "        if args.save_freq_int > 0:\n",
        "            output_dir = output_dir + f\"_save_int_{args.save_freq_int}\"\n",
        "        if args.noninvlr >= 0:\n",
        "            output_dir = output_dir + f\"_noninvlr_{args.noninvlr}\"\n",
        "    ### resuming is modified!!!\n",
        "    print(\"Preparing directory %s\" % output_dir)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    with open(os.path.join(output_dir, \"command.sh\"), \"w\") as f:\n",
        "        f.write(\" \".join(sys.argv))\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    print(\"Using model %s\" % args.model)\n",
        "    model_cfg = getattr(models, args.model)\n",
        "\n",
        "    print(\"Loading dataset %s from %s\" % (args.dataset, args.data_path))\n",
        "    transform_train = model_cfg.transform_test if args.no_aug else model_cfg.transform_train\n",
        "    loaders, num_classes = data.loaders(\n",
        "        args.dataset,\n",
        "        args.data_path,\n",
        "        args.batch_size,\n",
        "        args.num_workers,\n",
        "        transform_train,\n",
        "        model_cfg.transform_test,\n",
        "        use_validation=not args.use_test,\n",
        "        use_data_size = args.use_data_size,\n",
        "        split_classes=args.split_classes,\n",
        "        corrupt_train=args.corrupt_train\n",
        "    )\n",
        "\n",
        "    print(\"Preparing model\")\n",
        "    print(*model_cfg.args)\n",
        "\n",
        "    # add extra args for varying names\n",
        "    if 'ResNet18' in args.model:\n",
        "        extra_args = {'init_channels':args.num_channels}\n",
        "        if \"SI\" in args.model:\n",
        "            extra_args.update({'linear_norm':args.init_scale})\n",
        "    elif 'ConvNet' in args.model:\n",
        "        extra_args = {'init_channels':args.num_channels, 'max_depth':args.depth,'init_scale':args.init_scale}\n",
        "    elif args.model == 'LeNet':\n",
        "        extra_args = {'scale':args.scale}\n",
        "    else:\n",
        "        extra_args = {}\n",
        "        \n",
        "\n",
        "    model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs, **extra_args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    \n",
        "    param_groups = model.parameters()\n",
        "\n",
        "    if args.noninvlr >= 0:\n",
        "        param_groups = [\n",
        "            {'params': [p for n, p in model.named_parameters() if check_si_name(n, args.model)]},  # SI params are convolutions\n",
        "            {'params': [p for n, p in model.named_parameters() if not check_si_name(n, args.model)],'lr':args.noninvlr},  # other params\n",
        "        ]\n",
        "\n",
        "    optimizer = torch.optim.SGD(param_groups, \n",
        "                                lr=args.lr_init, \n",
        "                                momentum=args.momentum, \n",
        "                                weight_decay=args.wd)\n",
        "    \n",
        "    if args.cosan_schedule:\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "\n",
        "    epoch_from = args.resume_epoch + 1\n",
        "    epoch_to = epoch_from + args.epochs\n",
        "    print(f\"Training from {epoch_from} to {epoch_to - 1} epochs\")\n",
        "\n",
        "    if epoch_from > 0:\n",
        "        # Warning: due to specific lr schedule, resuming is generally not recommended!\n",
        "        print(f\"Loading checkpoint from the {args.resume_epoch} epoch\")\n",
        "        state = training_utils.load_checkpoint(resume_dir, args.resume_epoch)\n",
        "        model.load_state_dict(state['state_dict'])\n",
        "        optimizer.load_state_dict(state['optimizer'])\n",
        "        if args.noninvlr >= 0:\n",
        "            optimizer.param_groups[1][\"lr\"] = args.noninvlr\n",
        "        \n",
        "    si_pnorm_0 = None\n",
        "    if args.fix_si_pnorm:\n",
        "        if args.fix_si_pnorm_value > 0:\n",
        "            # No lr schedule, plz...\n",
        "            si_pnorm_0 = args.fix_si_pnorm_value\n",
        "        else:\n",
        "            si_pnorm_0 = np.sqrt(sum((p ** 2).sum().item() for n, p in model.named_parameters() if check_si_name(n, args.model)))\n",
        "            \n",
        "        print(f\"Fixing SI-pnorm to value {si_pnorm_0:.4f}\")\n",
        "\n",
        "\n",
        "    for epoch in range(epoch_from, epoch_to):\n",
        "        train_epoch(model, loaders, cross_entropy, optimizer, \n",
        "                    epoch=epoch, \n",
        "                    end_epoch=epoch_to, \n",
        "                    eval_freq=args.eval_freq, \n",
        "                    save_freq=args.save_freq,\n",
        "                    save_freq_int=args.save_freq_int,\n",
        "                    output_dir=output_dir,\n",
        "                    lr_init=args.lr_init,\n",
        "                    lr_schedule=not args.no_schedule,\n",
        "                    noninvlr=args.noninvlr,\n",
        "                    c_schedule=args.c_schedule,\n",
        "                    d_schedule=args.d_schedule,\n",
        "                    si_pnorm_0=si_pnorm_0,\n",
        "                    fbgd=args.fbgd,\n",
        "                    cosan_schedule = args.cosan_schedule)  \n",
        "        if args.cosan_schedule:\n",
        "            scheduler.step()\n",
        "    \n",
        "\n",
        "    print(\"model \", trial, \" done\")\n",
        "\n",
        "\n",
        "def train_epoch(model, loaders, criterion, optimizer, epoch, end_epoch,\n",
        "                eval_freq=1, save_freq=10, save_freq_int=0, output_dir='./', lr_init=0.01,\n",
        "                lr_schedule=True, noninvlr = -1, c_schedule=None, d_schedule=None, si_pnorm_0=None,fbgd=False, \n",
        "               cosan_schedule = False):\n",
        "\n",
        "    time_ep = time.time()\n",
        "\n",
        "    if not cosan_schedule:\n",
        "        if not lr_schedule:\n",
        "            lr = lr_init\n",
        "        elif c_schedule > 0:\n",
        "            lr = training_utils.c_schedule(epoch, lr_init, end_epoch, c_schedule)\n",
        "        elif d_schedule > 0:\n",
        "            lr = training_utils.d_schedule(epoch, lr_init, end_epoch, d_schedule)\n",
        "        else:\n",
        "            lr = training_utils.schedule(epoch, lr_init, end_epoch, swa=False)\n",
        "        if noninvlr >= 0:\n",
        "            training_utils.adjust_learning_rate_only_conv(optimizer, lr)\n",
        "        else:\n",
        "            training_utils.adjust_learning_rate(optimizer, lr)\n",
        "    else:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            lr = param_group[\"lr\"]\n",
        "            break\n",
        "\n",
        "    #train_res = training_utils.train_epoch(loaders[\"train\"], model, criterion, optimizer, fbgd=fbgd,si_pnorm_0=si_pnorm_0,\n",
        "    #                                       save_freq_int=save_freq_int,epoch = epoch,output_dir = output_dir)\n",
        "    train_res = training_utils.SAM_train_epoch(loaders[\"train\"], model, criterion, optimizer, r=0.05, fbgd=fbgd,si_pnorm_0=si_pnorm_0,\n",
        "                                           save_freq_int=save_freq_int,epoch = epoch,output_dir = output_dir)\n",
        "    if (\n",
        "        epoch == 0\n",
        "        or epoch % eval_freq == eval_freq - 1\n",
        "        or epoch == end_epoch - 1\n",
        "    ):\n",
        "        test_res = training_utils.eval(loaders[\"test\"], model, criterion)\n",
        "    else:\n",
        "        test_res = {\"loss\": None, \"accuracy\": None}\n",
        "        \n",
        "    def save_epoch(epoch):\n",
        "        training_utils.save_checkpoint(\n",
        "            output_dir,\n",
        "            epoch,\n",
        "            state_dict=model.state_dict(),\n",
        "            optimizer=optimizer.state_dict(),\n",
        "            train_res=train_res,\n",
        "            test_res=test_res\n",
        "        )\n",
        "\n",
        "    if save_freq is None:\n",
        "        if training_utils.do_report(epoch):\n",
        "            save_epoch(epoch)\n",
        "    elif epoch % save_freq == 0:\n",
        "        save_epoch(epoch)\n",
        "        \n",
        "    time_ep = time.time() - time_ep\n",
        "    values = [\n",
        "        epoch,\n",
        "        lr,\n",
        "        train_res[\"loss\"],\n",
        "        train_res[\"accuracy\"],\n",
        "        train_res[\"weights_norm\"],\n",
        "        test_res[\"loss\"],\n",
        "        test_res[\"accuracy\"],\n",
        "        time_ep,\n",
        "    ]\n",
        "    table = tabulate.tabulate([values], columns, tablefmt=\"simple\", floatfmt=\"8.4f\")\n",
        "    if epoch % 40 == 0:\n",
        "        table = table.split(\"\\n\")\n",
        "        table = \"\\n\".join([table[1]] + table)\n",
        "    else:\n",
        "        table = table.split(\"\\n\")[2]\n",
        "    print(table)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H0hBMGvlcp5",
        "outputId": "da4f4048-8e05-40e9-d5c9-3137f9aa097d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing base directory ./checkpoints\n",
            "Preparing directory ./checkpoints/trial_0\n",
            "Using model ConvNetSI\n",
            "Loading dataset CIFAR10 from ./CIFAR10\n",
            "Files already downloaded and verified\n",
            "Using train (45000) + validation (5000)\n",
            "Files already downloaded and verified\n",
            "Preparing model\n",
            "\n",
            "Training from 0 to 1000 epochs\n",
            "----  --------  ---------  --------  --------------  ---------  --------  --------\n",
            "  ep        lr    tr_loss    tr_acc    weights_norm    te_loss    te_acc      time\n",
            "----  --------  ---------  --------  --------------  ---------  --------  --------\n",
            "   0    0.1000    14.5218   16.8244        229.7722     2.5711   26.0000   26.2192\n",
            "   1    0.1000     2.5029   25.8800        222.4153                        25.1591\n",
            "   2    0.1000     2.0535   31.8733        215.3984                        24.7349\n",
            "   3    0.1000     1.8637   36.8289        208.7348                        24.7852\n",
            "   4    0.1000     1.7570   39.8867        202.3719                        24.9820\n",
            "   5    0.1000     1.8058   40.3511        196.4395                        24.7797\n",
            "   6    0.1000     1.7166   42.8156        190.7431                        24.8774\n",
            "   7    0.1000     1.5616   46.1422        185.2252                        24.8702\n",
            "   8    0.1000     1.5626   47.0956        180.0495                        24.7823\n",
            "   9    0.1000     1.5030   49.1844        175.1806     1.3711   52.3600   26.2438\n",
            "  10    0.1000     1.4388   50.5311        170.5576                        24.8660\n",
            "  11    0.1000     1.4063   52.0511        166.2519                        24.8671\n",
            "  12    0.1000     1.3675   53.6400        162.2228                        24.8210\n",
            "  13    0.1000     1.3028   55.5378        158.4113                        24.8204\n",
            "  14    0.1000     1.2359   58.0156        154.7686                        24.8306\n",
            "  15    0.1000     1.1983   59.3533        151.4352                        24.8384\n",
            "  16    0.1000     1.1863   59.6422        148.4352                        24.8875\n",
            "  17    0.1000     1.1093   62.2689        145.5285                        24.8943\n",
            "  18    0.1000     1.0308   64.6711        142.6903                        24.8120\n",
            "  19    0.1000     1.0378   65.2044        140.1590     0.9775   67.1200   26.1754\n",
            "  20    0.1000     1.0070   66.4089        137.8280                        25.0227\n",
            "  21    0.1000     0.9532   67.9089        135.5220                        24.9910\n",
            "  22    0.1000     0.8920   70.0222        133.2729                        24.8210\n",
            "  23    0.1000     0.9305   69.1178        131.4896                        24.8482\n",
            "  24    0.1000     0.9015   69.8711        129.8064                        24.7619\n",
            "  25    0.1000     0.8900   70.6422        128.2067                        24.8631\n",
            "  26    0.1000     0.8431   71.9311        126.6214                        24.8814\n",
            "  27    0.1000     0.8367   72.3244        125.2496                        24.8408\n",
            "  28    0.1000     0.8343   72.5400        124.0840                        24.9227\n",
            "  29    0.1000     0.7987   73.3978        122.8531     0.6668   77.6200   26.2926\n",
            "  30    0.1000     0.7700   74.5400        121.6168                        24.8294\n",
            "  31    0.1000     0.7649   74.6178        120.6037                        24.8307\n",
            "  32    0.1000     0.7543   75.1600        119.5563                        24.8361\n",
            "  33    0.1000     0.7310   75.7956        118.6648                        24.8427\n",
            "  34    0.1000     0.7243   76.0489        117.8217                        24.8873\n",
            "  35    0.1000     0.7290   75.9644        117.1961                        24.8697\n",
            "  36    0.1000     0.6979   76.7556        116.5279                        24.8129\n",
            "  37    0.1000     0.6941   76.9711        115.9595                        24.8234\n",
            "  38    0.1000     0.6772   77.6333        115.4196                        24.9369\n",
            "  39    0.1000     0.6569   78.1444        114.8454     0.7329   75.9800   26.3169\n",
            "----  --------  ---------  --------  --------------  ---------  --------  --------\n",
            "  ep        lr    tr_loss    tr_acc    weights_norm  te_loss    te_acc        time\n",
            "----  --------  ---------  --------  --------------  ---------  --------  --------\n",
            "  40    0.1000     0.6309   78.9800        114.3182                        24.8941\n",
            "  41    0.1000     0.6711   77.8444        114.0715                        24.8612\n",
            "  42    0.1000     0.6406   78.9333        113.6548                        24.8556\n",
            "  43    0.1000     0.6273   79.1578        113.2939                        24.8723\n",
            "  44    0.1000     0.6360   79.2067        113.1824                        24.8775\n",
            "  45    0.1000     0.6104   79.8733        112.9310                        24.8595\n",
            "  46    0.1000     0.6047   79.9467        112.7559                        24.8208\n",
            "  47    0.1000     0.5977   80.3311        112.4930                        24.7816\n",
            "  48    0.1000     0.5623   81.3489        112.1576                        24.8234\n",
            "  49    0.1000     0.5998   80.2778        112.1905     0.6177   80.1400   26.1597\n",
            "  50    0.1000     0.6045   80.3022        112.3524                        24.8475\n",
            "  51    0.1000     0.5845   80.9778        112.2912                        24.8117\n",
            "  52    0.1000     0.6170   80.1044        112.6207                        24.8674\n",
            "  53    0.1000     0.5661   81.3644        112.6032                        24.8043\n",
            "  54    0.1000     0.5448   82.0756        112.4297                        24.8903\n",
            "  55    0.1000     0.5434   81.9689        112.4862                        24.8480\n",
            "  56    0.1000     0.5605   81.8111        112.6048                        24.7892\n",
            "  57    0.1000     0.5372   82.3244        112.6175                        24.8329\n",
            "  58    0.1000     0.5956   80.8556        113.0687                        24.8170\n",
            "  59    0.1000     0.5653   81.6578        113.3488     0.5362   82.4400   26.1496\n",
            "  60    0.1000     0.5520   82.1000        113.5697                        24.8735\n",
            "  61    0.1000     0.5182   82.8911        113.4742                        24.8674\n",
            "  62    0.1000     0.5222   82.7000        113.5679                        24.9096\n",
            "  63    0.1000     0.5122   83.1178        113.6080                        24.9027\n",
            "  64    0.1000     0.5358   82.6178        113.8083                        24.9292\n",
            "  65    0.1000     0.5544   82.1044        114.2941                        24.9144\n",
            "  66    0.1000     0.4821   84.0178        114.1267                        24.9202\n",
            "  67    0.1000     0.5103   83.2889        114.3092                        24.8860\n",
            "  68    0.1000     0.4977   83.7889        114.3781                        24.8665\n",
            "  69    0.1000     0.5022   83.7489        114.5686     0.8048   77.1600   26.2068\n",
            "  70    0.1000     0.4842   84.1978        114.5639                        24.8915\n",
            "  71    0.1000     0.4745   84.4356        114.6415                        24.8686\n",
            "  72    0.1000     0.4823   84.2956        114.7609                        24.8525\n",
            "  73    0.1000     0.4810   84.2156        114.9851                        24.9051\n",
            "  74    0.1000     0.5095   83.5533        115.5663                        24.8747\n",
            "  75    0.1000     0.4798   84.3711        115.6961                        24.8866\n",
            "  76    0.1000     0.4651   84.8267        115.8089                        24.8714\n",
            "  77    0.1000     0.4951   84.1244        116.2386                        24.8892\n",
            "  78    0.1000     0.4571   85.3244        116.2274                        24.9014\n",
            "  79    0.1000     0.4551   85.1444        116.2938     0.5542   82.5200   26.2008\n",
            "----  --------  ---------  --------  --------------  ---------  --------  --------\n",
            "  ep        lr    tr_loss    tr_acc    weights_norm  te_loss    te_acc        time\n",
            "----  --------  ---------  --------  --------------  ---------  --------  --------\n",
            "  80    0.1000     0.4948   84.1089        116.7180                        24.9298\n",
            "  81    0.1000     0.4618   85.0044        116.9361                        24.8913\n",
            "  82    0.1000     0.4459   85.3422        116.9547                        24.8874\n",
            "  83    0.1000     0.4681   84.7667        117.2694                        24.8779\n",
            "  84    0.1000     0.4136   86.2622        117.0240                        24.8658\n",
            "  85    0.1000     0.4279   86.0489        117.0307                        24.8687\n",
            "  86    0.1000     0.4632   85.0533        117.4355                        24.8143\n",
            "  87    0.1000     0.4469   85.2867        117.7259                        24.8183\n",
            "  88    0.1000     0.4351   85.9089        117.8049                        24.8288\n",
            "  89    0.1000     0.4359   85.7444        117.9234     0.7540   78.4400   26.1369\n",
            "  90    0.1000     0.4478   85.6956        118.0986                        24.8618\n",
            "  91    0.1000     0.4523   85.4178        118.5254                        24.9102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5"
      ],
      "metadata": {
        "id": "mZE63B-lo5Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Managing experiments')\n",
        "parser.add_argument('--test', action='store_true',\n",
        "                        help='print (test) or os.system (run)')\n",
        "parser.add_argument('-f', help='service')\n",
        "\n",
        "args = parser.parse_args()\n",
        "    \n",
        "if args.test:\n",
        "    action = print\n",
        "else:\n",
        "    action = os.system\n",
        "\n",
        "ENVIRONMENT = 'CycleBNNet_env'"
      ],
      "metadata": {
        "id": "DqFC8ovwkf1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0CEXh2xkR6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}